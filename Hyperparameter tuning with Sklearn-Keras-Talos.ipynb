{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16ef3e4f85a17b9d756739c763555fcc5591ee07"
   },
   "outputs": [],
   "source": [
    "reviews_train = []\n",
    "for line in open('../input/movie_data/movie_data/full_train.txt', 'r'):\n",
    "    \n",
    "    reviews_train.append(line.strip())\n",
    "    \n",
    "reviews_test = []\n",
    "for line in open('../input/movie_data/movie_data/full_test.txt', 'r'):\n",
    "    \n",
    "    reviews_test.append(line.strip())\n",
    "    \n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75d321e34c7919e7c6b330fc6fc2193dae34056e"
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8e392d4fc9a5d126813b408d5725f7b3c123fb2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f0b9ddc020bcfe7861b2ea8e0041a1845ef3d12"
   },
   "outputs": [],
   "source": [
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4ed1b8cb135d9e74e0cf6d710a7022a2a9a7b64"
   },
   "outputs": [],
   "source": [
    "all_text = ' '.join([text for text in no_stop_words_train])\n",
    "print('Number of words in all_text:', len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e67148cb61e7428d2fb1903a8b5472b4f2596c57"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_text)\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a94dc0820953bf562e5b5eccf9afd4dd7f965fc1"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 5000\n",
    "max_words = 500\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(no_stop_words_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(no_stop_words_train)\n",
    "X_test = tokenizer.texts_to_sequences(no_stop_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76a734138d76a8ba247de115cc2113b894cf86b9"
   },
   "outputs": [],
   "source": [
    "#stopword_vectorizer = CountVectorizer()\n",
    "#stopword_vectorizer.fit(no_stop_words_train)\n",
    "#print(\"Size of dictionary: \", len(stopword_vectorizer.get_feature_names()))\n",
    "#print(\"Words in dictionary: \", stopword_vectorizer.get_feature_names())\n",
    "\n",
    "#X_train = stopword_vectorizer.transform(no_stop_words_train).toarray()\n",
    "#X_test = stopword_vectorizer.transform(no_stop_words_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60969bb2b7e3123e2a7cefbb87cf49fd9ca5187c"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba40a096e9038554f7e737a1d17fb9ecfb40226c"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, target, train_size = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b779cc10f7d36c2ccf3226ccfeed51944e44799"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa27426d90d77c2b17a9d00ca9f5d1f5b8d2da01"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPool1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58782be0ed67089c1a07abcca1adb7ac48c51c3c"
   },
   "outputs": [],
   "source": [
    "'''batch_size = [64, 128, 256]\n",
    "epochs = [10, 20, 30]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "neurons = [50, 100, 150, 200]\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer, learn_rate=learn_rate, \n",
    "                  dropout_rate=dropout_rate, weight_constraint=weight_constraint, momentum=momentum)'''\n",
    "\n",
    "neurons = [50, 100, 200]\n",
    "dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "optimizers = ['rmsprop', 'adam', 'sgd']\n",
    "batch_size = [64, 128, 256]\n",
    "epochs = [15, 30]\n",
    "param_grid = dict(neurons=neurons, optimizer=optimizers, dropout_rate=dropout, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af690a325c7bb7ea9a650d3fa45f1f928d351228"
   },
   "outputs": [],
   "source": [
    "'''def build_model(architecture='lstm'):\n",
    "    if architecture == 'embedding':\n",
    "        model = Sequential()\n",
    "        #model.add(Embedding(len(stopword_vectorizer.get_feature_names()), 32, input_length=max_words))\n",
    "        model.add(Embedding(vocab_size, 32, input_length=max_words))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    elif architecture == 'cnn':\n",
    "        model=Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "        model.add(MaxPool1D(pool_size = 2))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "    elif architecture == 'lstm':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "        model.add(LSTM(neurons, activation = 'tanh', recurrent_activation='hard_sigmoid', dropout=dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        raise NameError(\"Model name not found.\")\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e33cdbbebc54cada7f09c185fa78205ba5840f3"
   },
   "outputs": [],
   "source": [
    "def build_model(neurons, optimizer, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "    model.add(LSTM(neurons, activation='tanh', recurrent_activation='hard_sigmoid', dropout=dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e22c77005cf76791c85c7bfd0f45fa4ddcf3aaf"
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=build_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "537c3fd1df9e18a17eeb1bf3762f14d69d1c0e73"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=1)\n",
    "#grid_result = grid.fit(X_train, y_train)\n",
    "grid_result = grid.fit(X_train, y_train, callbacks=[reduce_lr, early_stopper])\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39e03bbd9d8f1f7b5445602cf5104320db5dd3eb"
   },
   "outputs": [],
   "source": [
    "'''history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, batch_size=128, verbose=1, callbacks=[reduce_lr, earlt_stopper]).history\n",
    "\n",
    "plt.plot(history['acc'], linewidth=2, label='Train')\n",
    "plt.plot(history['val_acc'], linewidth=2, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history['loss'], linewidth=2, label='Train')\n",
    "plt.plot(history['val_loss'], linewidth=2, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cae0191c9cf526158c92a12e297db980643a1a18"
   },
   "outputs": [],
   "source": [
    "!pip install talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cdb5a5f50f71c5323e780d5c8bc5f30aeffb34d"
   },
   "outputs": [],
   "source": [
    "import talos as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af39ff0bce693a9fe91f72187630711217dd1385"
   },
   "outputs": [],
   "source": [
    "def imdb_fn(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    dropout = float(params['dropout'])\n",
    "    lstm_neuron = int(params['lstm_neuron'])\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "    model.add(LSTM(lstm_neuron, activation='tanh', recurrent_activation='hard_sigmoid', dropout=dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=params['optimizer'],\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    out = model.fit(\n",
    "        x_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "        verbose=1,\n",
    "        validation_data=[x_val, y_val]\n",
    "    )\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b157a26f23c662b35fbef8bf083f1abab86d8ff9"
   },
   "outputs": [],
   "source": [
    "np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7406ebf80b97f8b2754e3d3ec81259f33413de90"
   },
   "outputs": [],
   "source": [
    "para = {\n",
    "    'epochs': [10, 15, 20],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'lstm_neuron': [100, 200, 300],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "scan_results = ta.Scan(X_train, np.array(y_train), params=para, model=imdb_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d9c4858261edffb7b13ae1c2fc22953d9cbbbb2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a89bae8e3259ddb92cce67eaa7bfb821b569e56e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9df1beab18af30c3a4df58e390ebd812d695abff"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
